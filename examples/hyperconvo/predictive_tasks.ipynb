{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()\n",
    "# os.chdir(\"../..\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Hypergraph features for various predictive tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake_cache = True\n",
    "# if remake_cache:\n",
    "#     hc = convokit.HyperConvo()\n",
    "#     hc.fit_transform(corpus)\n",
    "#     feats = corpus.get(\"meta\")[\"hyperconvo\"]\n",
    "\n",
    "#     with open(\"feats.p\", \"wb\") as f:\n",
    "#         pickle.dump(feats, f)\n",
    "# else:\n",
    "#     with open(\"feats.p\", \"rb\") as f:\n",
    "#         feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n",
    "hc.fit_transform(corpus)\n",
    "feats = corpus.meta[\"hyperconvo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e58slx0', 'e594ur8', 'e5988ip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(feats.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first 10 comments in each thread\n",
    "thread_pfxs = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_roots_by_self_post = defaultdict(list)\n",
    "for top_level_comment, thread in threads.items():\n",
    "    thread_roots_by_self_post[thread[top_level_comment].root].append(top_level_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8286"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thread_roots_by_self_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task comment-growth\n",
      "- 174 positive, 174 negative pts\n",
      "- hyperconvo: 0.7252 train, 0.5143 test\n",
      "- volume: 0.5687 train, 0.6000 test\n",
      "- reply tree: 0.6102 train, 0.5143 test\n",
      "- BOW: 0.9936 train, 0.4286 test\n",
      "task commenter-growth\n",
      "- 121 positive, 121 negative pts\n",
      "- hyperconvo: 0.6866 train, 0.6800 test\n",
      "- volume: 0.4747 train, 0.4000 test\n",
      "- reply tree: 0.5668 train, 0.6000 test\n",
      "- BOW: 0.9954 train, 0.4400 test\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "for task in [\"comment-growth\", \"commenter-growth\"]: #, \"post-deleted\", \"user-deleted\"\n",
    "    print(\"task {}\".format(task))\n",
    "    pos, neg = [], []\n",
    "    for roots in thread_roots_by_self_post.values():\n",
    "        has_pos, has_neg = [], []\n",
    "        for root in roots:\n",
    "            if task == \"comment-growth\":\n",
    "                if len(threads[root]) >= 15:\n",
    "                    has_pos.append(root)\n",
    "                if len(threads[root]) == 10:\n",
    "                    has_neg.append(root)\n",
    "            elif task == \"commenter-growth\":\n",
    "                if len(threads[root]) >= 20:\n",
    "                    if len(set(c.user.name for c in threads[root].values())) >= \\\n",
    "                        len(set(c.user.name for c in thread_pfxs[root].values())) * 2:\n",
    "                            has_pos.append(root)\n",
    "                    else:\n",
    "                        has_neg.append(root)\n",
    "#             elif task == \"post-deleted\":\n",
    "#                 if len(threads[root]) >= 10:\n",
    "#                     if threads[root][root].user.info[\"post-deleted\"]:\n",
    "#                         has_pos.append(root)\n",
    "#                     else:\n",
    "#                         has_neg.append(root)\n",
    "#             elif task == \"user-deleted\":\n",
    "#                 if len(threads[root]) >= 10:\n",
    "#                     if threads[root][root].user.info[\"user-deleted\"]:\n",
    "#                         has_pos.append(root)\n",
    "#                     else:\n",
    "#                         has_neg.append(root)\n",
    "            else:\n",
    "                print(\"unrecognized task name\")\n",
    "\n",
    "        if has_pos and has_neg:\n",
    "            pos.append(random.choice(has_pos))\n",
    "            neg.append(random.choice(has_neg))\n",
    "\n",
    "    print(\"- {} positive, {} negative pts\".format(len(pos), len(neg)))\n",
    "\n",
    "    # make data from pos and neg\n",
    "    X = []\n",
    "    X_volume, X_reply, X_bow = [], [], []\n",
    "    threads_text = []\n",
    "    for root in pos + neg:\n",
    "        # get ordered set of feature values\n",
    "        v = [feats[root][k] for k in sorted(feats[root].keys()) if \"TRIADS\" not in k]\n",
    "        # data cleaning\n",
    "        v = [t if (not np.isnan(t) and np.isfinite(t)) else 0 for t in v]\n",
    "        X.append(v)\n",
    "        \n",
    "        # volume baseline - get num participants in thread with at least length of 10\n",
    "        X_volume.append([len(set(c.user.name for c in thread_pfxs[root].values()))])   \n",
    "        # reply tree baseline\n",
    "        X_reply.append([feats[root][k] if (not np.isnan(feats[root][k]) and np.isfinite(feats[root][k])) else 0 for k in sorted(feats[root].keys()) \n",
    "                        if \"c->c\" in k])\n",
    "        # BOW baseline text\n",
    "        thread_text = \" \".join([u.text for u in thread_pfxs[root].values()\n",
    "                                if not (task == \"post-deleted\" and u.id == root)])  \n",
    "        # don't consider root post for post-deleted task, since we could just look for the string \"[deleted]\"\n",
    "        threads_text.append(thread_text)\n",
    "        \n",
    "    ys = [1]*len(pos) + [0]*len(neg)\n",
    "\n",
    "    X, ys = np.array(X), np.array(ys)\n",
    "\n",
    "    for X_tmp, name in [(X, \"hyperconvo\"), (X_volume, \"volume\"), (X_reply, \"reply tree\"), (None, \"BOW\")]:\n",
    "        if name == \"BOW\":\n",
    "            text_train, text_test, y_train, y_test = train_test_split(threads_text, ys, test_size=0.1, random_state=42)\n",
    "            cv = CountVectorizer(min_df=0.05, max_df=0.8)\n",
    "            X_train = cv.fit_transform(text_train)\n",
    "            X_test = cv.transform(text_test)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_tmp, ys, test_size=0.1, random_state=42)\n",
    "        \n",
    "        clf = LogisticRegression(solver=\"liblinear\")\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_acc = clf.score(X_train, y_train)\n",
    "        test_acc = clf.score(X_test, y_test)\n",
    "        print(\"- {}: {:.4f} train, {:.4f} test\".format(name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperconvo-venv",
   "language": "python",
   "name": "hyperconvo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
