{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikiconv Corpus Deletion Statistics \n",
    "This notebook provides a way to compute comment deletion statistics similar to the one presented in the corresponding Wikiconv paper (http://www.cs.cornell.edu/~cristian/index_files/wikiconv-conversation-corpus.pdf), which presents a corpus of the complete history of conversations on Wikipedia.\n",
    "\n",
    "This notebook will output comment deletion statistics for a small subset of Corpus data and showcase the form of provided utterances. \n",
    "For reference, deleted comments can have three forms:\n",
    "1. Normal Comments - have toxicity < 0.5 and sever_toxicity < 0.5\n",
    "2. Toxic Comments - have toxicity > 0.5\n",
    "3. Severe Toxic Comments - have sever_toxicity > 0.5\n",
    "\n",
    "The deletion rates of the comments should follow this pattern as well for provided deletion time intervals up to 365 days:\n",
    "1. Normal Comments - deleted at the lowest rate\n",
    "2. Toxic comments - deleted at a rate greater than Normal Comments but less than Severe Toxic Comments\n",
    "3. Severe Toxic Comments - deleted at the greatest rate\n",
    "\n",
    "Finally, two important points to consider are that deletion time intervals greater than 365 days may inflate numbers due to periodic page cleanups and that individual corpora may show different deletion rates than described above due to variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "from datetime import datetime, timedelta\n",
    "from convokit import Corpus, User, Utterance, Conversation, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/jonathan/.convokit/downloads/wikiconv-2003\n"
     ]
    }
   ],
   "source": [
    "# Load the 2003 wikiconv corpus (feel free to change this to a year of your preference)\n",
    "wikiconv_corpus = Corpus(filename=download('wikiconv-2003'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic facts about this subset of the corpus: 91,787 conversations and 140,265 utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91787"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(wikiconv_corpus.iter_conversations()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140265"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(wikiconv_corpus.iter_utterances()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each utterance has the following structure:\n",
    "In this case the modification, deletion, and restoration lists are empty, but in cases where actions occur on the original comment, they will be filled with utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': '5021479.2081.2077', 'user': User([('name', 'Jay')]), 'root': '5021479.1277.1272', 'reply_to': '5021479.1277.1272', 'timestamp': 1070614595.0, 'text': \"You're right about separating the sandwich of war names and MiG names. Each plane should be sorted chronologically and have its own sentence detailing its importance. \", 'meta': {'is_section_header': True, 'indentation': '2', 'toxicity': 0.1219038, 'sever_toxicity': 0.06112729, 'ancestor_id': '5021479.2081.2077', 'rev_id': '5021479', 'parent_id': None, 'original': None, 'modification': [], 'deletion': [], 'restoration': []}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wikiconv_corpus.iter_utterances())[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a function that takes as input the deletion list and returns the count of the different types of  deletion instances (normal, toxic, and severe toxic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_deletion_list_data(list_of_deletion_utterances, original_posting_time, timedelta_value):\n",
    "    #Count the total number of deleted utterances of each type\n",
    "    count_normal = 0\n",
    "    count_toxic= 0\n",
    "    count_sever_toxic = 0\n",
    "    \n",
    "    for deletion_utt in list_of_deletion_utterances:\n",
    "        toxicity_val = deletion_utt.meta['toxicity']\n",
    "        sever_toxicity_val = deletion_utt.meta['sever_toxicity']\n",
    "        timestamp_value = deletion_utt.timestamp\n",
    "        deletion_datetime_val = datetime.fromtimestamp(timestamp_value)\n",
    "        \n",
    "        #delta_value is the time delta between when the deletion utt happened and the original utt's posting \n",
    "        if (original_posting_time is None):\n",
    "            delta_value = 0\n",
    "        else: \n",
    "            delta_value =  deletion_datetime_val - original_posting_time \n",
    "        \n",
    "        #If the delta value is less than the provided time delta, consider its type\n",
    "        if (delta_value <= timedelta(days = timedelta_value)):\n",
    "                if (toxicity_val < 0.5 and sever_toxicity_val < 0.5):\n",
    "                    count_normal +=1\n",
    "                if (toxicity_val > 0.5):\n",
    "                    count_toxic +=1\n",
    "                if (sever_toxicity_val > 0.5):\n",
    "                    count_sever_toxic +=1 \n",
    "    \n",
    "    #Return in  tuple  form the number of each type of affected comment\n",
    "    return (count_normal, count_toxic, count_sever_toxic)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute how many of each type of comment are deleted in the list of total utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deletion_counts(individual_utterance_list, timedelta_value):\n",
    "    #Normal Data count\n",
    "    count_normal_deleted = 0\n",
    "    count_normal_total = 0\n",
    "    set_of_normal_comments = set()\n",
    "    \n",
    "\n",
    "    #Toxic data count\n",
    "    count_toxic_deleted = 0\n",
    "    count_toxic_total = 0\n",
    "    set_of_toxic_comments = set()\n",
    "\n",
    "    #Sever Toxic Data count\n",
    "    count_sever_deleted = 0\n",
    "    count_sever_total = 0\n",
    "    set_of_sever_comments = set()\n",
    "    \n",
    "    #Check each utterance\n",
    "    for utterance_value in individual_utterance_list:\n",
    "        toxicity_val = utterance_value.meta['toxicity']\n",
    "        sever_toxicity_val = utterance_value.meta['sever_toxicity']\n",
    "        \n",
    "        #Find the total number of comments of each type\n",
    "        if (toxicity_val < 0.5 and sever_toxicity_val < 0.5):\n",
    "            if (utterance_value.id not in set_of_normal_comments):\n",
    "                count_normal_total +=1     \n",
    "                set_of_normal_comments.add(utterance_value.id)  \n",
    "                \n",
    "        if (toxicity_val > 0.5):\n",
    "            if (utterance_value.id not in set_of_toxic_comments):\n",
    "                count_toxic_total +=1\n",
    "                set_of_toxic_comments.add(utterance_value.id)\n",
    "   \n",
    "        if (sever_toxicity_val > 0.5):\n",
    "            if (utterance_value.id not in set_of_sever_comments):\n",
    "                count_sever_total +=1\n",
    "                set_of_sever_comments.add(utterance_value.id)\n",
    "                \n",
    "        #Find the time that the original utterance is posted\n",
    "        original_utterance = utterance_value.meta['original']\n",
    "        if (original_utterance is not None):\n",
    "            original_time = original_utterance.timestamp\n",
    "            original_date_time = datetime.fromtimestamp(original_time)\n",
    "        else:\n",
    "            original_date_time =  datetime.fromtimestamp(utterance_value.timestamp)\n",
    "        \n",
    "        \n",
    "        #Count the number of deleted comments \n",
    "        if (len(utterance_value.meta['deletion']) >0):\n",
    "            deletion_list = utterance_value.meta['deletion']\n",
    "            ind_normal, ind_toxic, ind_sever = check_deletion_list_data(deletion_list, original_date_time, timedelta_value)\n",
    "            count_normal_deleted += ind_normal\n",
    "            count_toxic_deleted += ind_toxic\n",
    "            count_sever_deleted += ind_sever\n",
    "    \n",
    "    return (count_normal_deleted, count_toxic_deleted, count_sever_deleted, \n",
    "            count_normal_total, count_toxic_total, count_sever_total)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we willl define a method to print out the different statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(count_normal_deleted, count_toxic_deleted, count_sever_deleted, total_normal, total_toxic, total_sever):\n",
    "    prop_normal = count_normal_deleted/float(total_normal)\n",
    "    prop_toxic = count_toxic_deleted/float(total_toxic)\n",
    "    prop_sever = count_sever_deleted/float(total_sever)\n",
    "\n",
    "    print ('Proportion of normal comments deleted: ' + str(prop_normal))  \n",
    "    print ('Proportion of toxic comments deleted: ' + str(prop_toxic))\n",
    "    print ('Proportion of sever toxic comments deleted: ' + str(prop_sever))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can process the corpus and output the deletion statistics in the one day interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of normal comments deleted: 0.04678097466457062\n",
      "Proportion of toxic comments deleted: 0.07529684332464523\n",
      "Proportion of sever toxic comments deleted: 0.0836092715231788\n"
     ]
    }
   ],
   "source": [
    "#Set the default values we will need to compute the corpus statistics\n",
    "individual_utterance_list = list(wikiconv_corpus.iter_utterances())\n",
    "len_utterances = len(individual_utterance_list)\n",
    "timedelta_value = 1\n",
    "\n",
    "#Find the counts of deleted comments and print statistics with a time delta of One Day\n",
    "(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    " total_normal, total_toxic, total_sever) = get_deletion_counts(individual_utterance_list, timedelta_value)\n",
    "print_statistics(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    "                 total_normal,  total_toxic, total_sever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify the time delta in days  (which considers comments that are deleted up to that time delta value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of normal comments deleted: 0.10941395824955215\n",
      "Proportion of toxic comments deleted: 0.1448016217781639\n",
      "Proportion of sever toxic comments deleted: 0.14072847682119205\n"
     ]
    }
   ],
   "source": [
    "timedelta_value = 7\n",
    "(count_normal_deleted, count_toxic_deleted, count_sever_deleted, \n",
    " total_normal, total_toxic, total_sever) = get_deletion_counts(individual_utterance_list, timedelta_value)\n",
    "print_statistics(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    "                 total_normal,  total_toxic, total_sever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change to the 30 day view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of normal comments deleted: 0.18123788981098965\n",
      "Proportion of toxic comments deleted: 0.24123950188242108\n",
      "Proportion of sever toxic comments deleted: 0.23178807947019867\n"
     ]
    }
   ],
   "source": [
    "timedelta_value = 30\n",
    "(count_normal_deleted, count_toxic_deleted, count_sever_deleted, \n",
    " total_normal, total_toxic, total_sever) = get_deletion_counts(individual_utterance_list, timedelta_value)\n",
    "print_statistics(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    "                 total_normal,  total_toxic, total_sever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the 365 day time delta view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of normal comments deleted: 0.2552188059810624\n",
      "Proportion of toxic comments deleted: 0.32696206197509414\n",
      "Proportion of sever toxic comments deleted: 0.3129139072847682\n"
     ]
    }
   ],
   "source": [
    "timedelta_value = 365\n",
    "(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    " total_normal, total_toxic, total_sever) = get_deletion_counts(individual_utterance_list, timedelta_value)\n",
    "print_statistics(count_normal_deleted, count_toxic_deleted, count_sever_deleted,\n",
    "                 total_normal,  total_toxic, total_sever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
